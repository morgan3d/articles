**Fantastic Machines**

In this article I review what it means for a computer system ("machine") to be real or virtual.


Fantasy console, virtual machine, emulator, etc.

Emulator: simulate a real machine with a software program running on a different machine.
Typically done for backwards compatibility, e.g., a PS2 emulator running on a PC.
The emulated machine is often less expressive, lower precision, etc. than the host machine.
But not always!

For example, also done by processor vendors when simulating their future products. Because Java was originally intended as a language for an embedded Java processor that Sun would produce, the "Java Virtual Machine" isn't a traditional interpreter or runtime, but can be viewed as this class of emulator!

fantasy = impractical or capricous
fantasy = ideal


Another example of "emulating upwards" is a common rite of passage course among EECS students:
building a physical 8-bit machine that then emulates a specific 16- or higher bit (8086, Motorola 68000, and DEC Alpha are common choices).

Sxsoftware-defined hardware


Virtual machine: software emulating a machine on another machine with comparable architecture, but provisioned differently (e.g., running on a physical machine with more memory) to allow sandboxed instances from multiple users to run simultaneously as if each had full control of the machine.
This provides security, avoids conflicts between configurations, and enables cost effective server scaling. It also allows migration of whole instances from one physical machine to another while running by serializing the entire VM, which 
has applications for load balancing and continuous uptime.

Fantasy machine: use a software program running on one machine to simulate one *that never existed*, and that there is no intention of producing. pico8, etc. Generalizing to "machine" from "console"


The Present Fantasy
==============================

Transmeta

Modern Intel and AMD x86 based computers are actually fantasy machines. The baseline x86 architecture does not match how these processors actually operate, and has not for many years.
The instructions and registers are entirely simulated and the "machine code" of programs is actually interpreted or transcoded at runtime. So, there is a fantasy specification for a modern x86-64 processor using AVX, etc., and there are physical processors that implement that specification through an emulation layer.

Wine: A windows x86 machine running on a linux x86 machine.

OpenGL, DirectX, Vulkan, CUDA assembly/machine code and the machine models described in terms of registers, fixed function units, etc. are an abstraction layer.

Virtual memory



Video Games
==============================
pico8, tic80, etc. for fun

for reasons described above, the GPU and CPU interfaces to modern consoles are in fact fantasy specifications.

Future is cloud fantasy consoles!


Quantum Computers
==============================


Artificial Intelligence
==============================

Conscious level separate from computation level. 


Does this apply to today's deep learning systems? I believe so. To make a clear claim
amid the field's varying terminology, let me step back and define some terms.

In academic research, "artificial intelligence" is generally held to a higher level
of autonomy, and ideally consciousness, than "machine learning", which is considered
to be a way of building pattern recognition data structures from data instead of by hand.

Machine learning began as a subfield of artificial intelligence and is a strong candidate for
one of the building blocks of a true artificial intelligence, but today is considered a
separate problem and useful in its own right.

Within machine learning, a specific family of methods called "neural nets" use graphs of
nonlinear filters inspired by biological neural systems. The edge weights in the graph are
tuned ("trained") by the backpropagation algorithm, which is a form of the hill climbing
heuristic: to get to the highest point, go up the steepest hill. The application of these to
relatively large networks and training data sets is called "deep learning" and has recently
become extremely successful due to highly parallel GPU computation and large, fast memory
systems (GDDR5, GDDR6, and HBM). That success is manifest in this deep learning approach
replacing hand-created data structures for recommendation, image recognition, vehicle control
(i.e., "self-driving car"), video game character, weather prediction, and other systems across
the industry.


...


Virtual Reality
==============================

http://voicesofvr.com/667-janusvrs-vesta-hub-is-the-beginnings-of-the-decentralized-metaverse/

